#!/usr/bin/env python3
"""
æ™ºè°±GLM API è¿æ¥æµ‹è¯•è„šæœ¬

æµ‹è¯•æ™ºè°±GLM APIçš„è¿æ¥æ€§å’Œå¯ç”¨æ€§
"""

import os
import asyncio
from openai import AsyncOpenAI

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®æˆ–ä½¿ç”¨æä¾›çš„å¯†é’¥
GLM_API_KEY = os.getenv("GLM_API_KEY", "15a2323847724a62a48280976135dbe7.TnFrReicNY7tTL1p")
GLM_BASE_URL = os.getenv("GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
GLM_MODEL = os.getenv("GLM_DEFAULT_MODEL", "glm-4")

print("=" * 60)
print("ğŸ§ª æ™ºè°±GLM API è¿æ¥æµ‹è¯•")
print("=" * 60)
print(f"ğŸ”‘ API Key: {GLM_API_KEY[:20]}...")
print(f"ğŸŒ Base URL: {GLM_BASE_URL}")
print(f"ğŸ¤– æ¨¡å‹: {GLM_MODEL}")
print("=" * 60)

async def test_glm_connection():
    """æµ‹è¯•æ™ºè°±GLM APIè¿æ¥"""
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=GLM_API_KEY,
        base_url=GLM_BASE_URL
    )
    
    print("\n1ï¸âƒ£ æµ‹è¯•APIè¿æ¥å’Œè®¤è¯...")
    try:
        # æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨
        models = await client.models.list()
        print(f"âœ… è¿æ¥æˆåŠŸï¼å‘ç° {len(models.data)} ä¸ªå¯ç”¨æ¨¡å‹")
        
        # æ˜¾ç¤ºå‰5ä¸ªæ¨¡å‹
        print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆå‰5ä¸ªï¼‰:")
        for i, model in enumerate(models.data[:5]):
            print(f"   {i+1}. {model.id}")
            
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return False
    
    print(f"\n2ï¸âƒ£ æµ‹è¯•æ¨¡å‹ '{GLM_MODEL}' å¯¹è¯åŠŸèƒ½...")
    try:
        # æµ‹è¯•ç®€å•å¯¹è¯
        response = await client.chat.completions.create(
            model=GLM_MODEL,
            messages=[
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
            ],
            max_tokens=100,
            temperature=0.7
        )
        
        content = response.choices[0].message.content
        tokens_used = response.usage.total_tokens if response.usage else "æœªçŸ¥"
        
        print(f"âœ… å¯¹è¯æµ‹è¯•æˆåŠŸï¼")
        print(f"ğŸ“ å›å¤å†…å®¹: {content[:100]}...")
        print(f"ğŸ¯ Tokenä½¿ç”¨: {tokens_used}")
        
    except Exception as e:
        print(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print(f"\n3ï¸âƒ£ æµ‹è¯•æµå¼å“åº”...")
    try:
        # æµ‹è¯•æµå¼å¯¹è¯
        stream = await client.chat.completions.create(
            model=GLM_MODEL,
            messages=[
                {"role": "user", "content": "è¯·æ•°1åˆ°5"}
            ],
            max_tokens=50,
            temperature=0.1,
            stream=True
        )
        
        print("âœ… æµå¼å“åº”æµ‹è¯•:")
        chunks_received = 0
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(f"   ğŸ“¦ æ”¶åˆ°å—: '{content}'")
                chunks_received += 1
        
        print(f"âœ… æµå¼æµ‹è¯•æˆåŠŸï¼å…±æ”¶åˆ° {chunks_received} ä¸ªæ•°æ®å—")
        
    except Exception as e:
        print(f"âŒ æµå¼æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print(f"\n4ï¸âƒ£ æµ‹è¯•é”™è¯¯å¤„ç†...")
    try:
        # æµ‹è¯•æ— æ•ˆæ¨¡å‹
        await client.chat.completions.create(
            model="invalid-model-name",
            messages=[
                {"role": "user", "content": "æµ‹è¯•"}
            ],
            max_tokens=10
        )
        print("âš ï¸  é”™è¯¯å¤„ç†æµ‹è¯•: åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
        
    except Exception as e:
        print(f"âœ… é”™è¯¯å¤„ç†æ­£å¸¸: {type(e).__name__}")
    
    return True

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        success = await test_glm_connection()
        
        print("\n" + "=" * 60)
        if success:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºè°±GLM APIé…ç½®æ­£ç¡®")
            print("âœ… å¯ä»¥ç»§ç»­è¿›è¡ŒProviderç³»ç»Ÿæµ‹è¯•")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥APIé…ç½®")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. éªŒè¯API Keyæ˜¯å¦æœ‰æ•ˆ")
            print("   2. æ£€æŸ¥Base URLæ˜¯å¦æ­£ç¡®")
            print("   3. ç¡®è®¤æ¨¡å‹åç§°æ˜¯å¦æ”¯æŒ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())