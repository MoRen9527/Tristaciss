# AIå•èŠåŠŸèƒ½æ·±åº¦è§£æ

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†AIå•èŠåŠŸèƒ½çš„å®Œæ•´å®ç°ï¼Œä»ç»„ä»¶åˆå§‹åŒ–ã€é…ç½®åŠ è½½ã€æ¨¡å‹æ£€æµ‹åˆ°æ¶ˆæ¯å‘é€ã€æµå¼å“åº”å¤„ç†ã€tokenç»Ÿè®¡ç­‰æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚ã€‚é€šè¿‡æœ¬æ–‡æ¡£ï¼Œå¼€å‘è€…å¯ä»¥å®Œå…¨ç†è§£å•èŠåŠŸèƒ½çš„å·¥ä½œåŸç†ï¼Œå¹¶å…·å¤‡é‡æ–°å¼€å‘ç±»ä¼¼åŠŸèƒ½çš„èƒ½åŠ›ã€‚

## ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
2. [ç»„ä»¶åˆå§‹åŒ–æµç¨‹](#ç»„ä»¶åˆå§‹åŒ–æµç¨‹)
3. [é…ç½®ç®¡ç†ä¸æ¨¡å‹æ£€æµ‹](#é…ç½®ç®¡ç†ä¸æ¨¡å‹æ£€æµ‹)
4. [æ¶ˆæ¯å‘é€æµç¨‹](#æ¶ˆæ¯å‘é€æµç¨‹)
5. [åç«¯å¤„ç†æœºåˆ¶](#åç«¯å¤„ç†æœºåˆ¶)
6. [æä¾›å•†å…¼å®¹æ€§åˆ¤æ–­](#æä¾›å•†å…¼å®¹æ€§åˆ¤æ–­)
7. [Tokenç»Ÿè®¡ä¸è´¹ç”¨è®¡ç®—](#tokenç»Ÿè®¡ä¸è´¹ç”¨è®¡ç®—)
8. [æµå¼å“åº”ä¸æ‰“å­—æœºæ•ˆæœ](#æµå¼å“åº”ä¸æ‰“å­—æœºæ•ˆæœ)
9. [é”™è¯¯å¤„ç†æœºåˆ¶](#é”™è¯¯å¤„ç†æœºåˆ¶)
10. [æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–](#æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–)

---

## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### æŠ€æœ¯æ ˆ
- **å‰ç«¯**: React.js + TypeScript + Material-UI + Redux Toolkit
- **åç«¯**: FastAPI + Python + å¼‚æ­¥å¤„ç†
- **AIæä¾›å•†**: OpenRouter, OpenAI, DeepSeek, GLMç­‰
- **çŠ¶æ€ç®¡ç†**: Redux + RTK Query
- **é€šä¿¡åè®®**: HTTP + JSON
- **å®æ—¶æ›´æ–°**: æ‰“å­—æœºæ•ˆæœ + å®šæ—¶åŒæ­¥

### æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾
```
ChatPanel.tsx (ä¸»èŠå¤©é¢æ¿)
    â†“ åˆå§‹åŒ–
ConfigManager.ts (é…ç½®ç®¡ç†å™¨)
    â†“ åŠ è½½é…ç½®
ModelSelectionDialog.tsx (æ¨¡å‹é€‰æ‹©å™¨)
    â†“ é€‰æ‹©æ¨¡å‹
MessageList.tsx (æ¶ˆæ¯åˆ—è¡¨)
    â†“ å‘é€æ¶ˆæ¯
chatSlice.ts (ReduxçŠ¶æ€ç®¡ç†)
    â†“ HTTPè¯·æ±‚
api.js (APIæœåŠ¡å±‚)
    â†“ åç«¯è·¯ç”±
fastapi_stream.py (æ¶ˆæ¯å¤„ç†)
    â†“ æä¾›å•†é€‰æ‹©
ProviderManager (æä¾›å•†ç®¡ç†å™¨)
    â†“ æ¨¡å‹è°ƒç”¨
providers/*.py (å…·ä½“æä¾›å•†å®ç°)
    â†“ å“åº”å¤„ç†
Tokenç»Ÿè®¡ + è´¹ç”¨è®¡ç®— + å‰ç«¯æ˜¾ç¤º
```

---

## ç»„ä»¶åˆå§‹åŒ–æµç¨‹

### 2.1 ChatPanelç»„ä»¶åˆå§‹åŒ–

**æ–‡ä»¶**: `avatar-react/src/components/chat/ChatPanel.tsx`

#### 2.1.1 ç»„ä»¶çŠ¶æ€åˆå§‹åŒ–

```typescript
const ChatPanel: React.FC = () => {
  const dispatch = useAppDispatch();
  const { 
    messages, 
    loading: isLoading, 
    chatMode,
    selectedProvider,
    selectedModel,
    availableProviders,
    providersLoading,
    groupChatSettings,
    error 
  } = useAppSelector(state => state.chat);

  // æœ¬åœ°çŠ¶æ€
  const [inputMessage, setInputMessage] = useState('');
  const [availableModels, setAvailableModels] = useState<string[]>([]);
  const [loadingModels, setLoadingModels] = useState(false);
  const [typingMessageId, setTypingMessageId] = useState<string | null>(null);
  const [usdToCnyRate, setUsdToCnyRate] = useState<number>(7.2);
  const messagesEndRef = useRef<HTMLDivElement>(null);
```

**çŠ¶æ€è¯´æ˜**ï¼š
- `messages`: Reduxç®¡ç†çš„æ¶ˆæ¯å†å²
- `selectedProvider/selectedModel`: å½“å‰é€‰æ‹©çš„æä¾›å•†å’Œæ¨¡å‹
- `availableProviders/availableModels`: å¯ç”¨çš„æä¾›å•†å’Œæ¨¡å‹åˆ—è¡¨
- `typingMessageId`: å½“å‰æ­£åœ¨æ‰“å­—æœºæ•ˆæœçš„æ¶ˆæ¯ID
- `usdToCnyRate`: åŠ¨æ€æ±‡ç‡ï¼Œç”¨äºè´¹ç”¨è®¡ç®—

#### 2.1.2 æ±‡ç‡åˆå§‹åŒ–

```typescript
// åˆå§‹åŒ–æ±‡ç‡
useEffect(() => {
  const initExchangeRate = async () => {
    try {
      const rate = getUsdToCnyRate();
      setUsdToCnyRate(rate);
    } catch (error) {
      console.warn('è·å–æ±‡ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ 7.2:', error);
    }
  };
  
  initExchangeRate();
  
  // æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡æ±‡ç‡
  const interval = setInterval(initExchangeRate, 60 * 60 * 1000);
  return () => clearInterval(interval);
}, []);
```

**æ±‡ç‡æœåŠ¡**: `avatar-react/src/services/exchangeRate.ts`
```typescript
export const getUsdToCnyRate = async (): Promise<number> => {
  try {
    // ä¼˜å…ˆä»åç«¯APIè·å–
    const response = await fetch('/api/exchange-rate');
    const data = await response.json();
    
    if (data.success && data.rate) {
      // ç¼“å­˜åˆ°localStorage
      localStorage.setItem('usd_to_cny_rate', JSON.stringify({
        rate: data.rate,
        timestamp: Date.now()
      }));
      return data.rate;
    }
  } catch (error) {
    console.warn('ä»åç«¯è·å–æ±‡ç‡å¤±è´¥:', error);
  }
  
  // å›é€€åˆ°ç¼“å­˜æˆ–é»˜è®¤å€¼
  const cached = localStorage.getItem('usd_to_cny_rate');
  if (cached) {
    const { rate, timestamp } = JSON.parse(cached);
    // ç¼“å­˜æœ‰æ•ˆæœŸ24å°æ—¶
    if (Date.now() - timestamp < 24 * 60 * 60 * 1000) {
      return rate;
    }
  }
  
  return 7.2; // é»˜è®¤æ±‡ç‡
};
```

---

## é…ç½®ç®¡ç†ä¸æ¨¡å‹æ£€æµ‹

### 3.1 æä¾›å•†é…ç½®åŠ è½½

#### 3.1.1 æŒ‡ä»¤æ¨¡å‹é…ç½®åŠ è½½

```typescript
// åŠ è½½å¯ç”¨çš„æä¾›å•†
useEffect(() => {
  const loadProviders = async () => {
    if (availableProviders.length === 0 && !providersLoading) {
      console.log('ğŸ”„ ChatPanel: å¼€å§‹ä½¿ç”¨æŒ‡ä»¤æ¨¡å‹åŠ è½½æä¾›å•†é…ç½®...');
      dispatch(setProvidersLoading(true));
      
      try {
        // å®Œå…¨æ¸…ç†æ‰€æœ‰å¯èƒ½çš„ç¼“å­˜
        localStorage.removeItem('provider_settings');
        localStorage.removeItem('group_chat_settings');
        localStorage.removeItem('chat_history');
        
        // å¼ºåˆ¶æ¸…ç†ConfigManagerç¼“å­˜
        configManager.cache.clear();
        
        // å…ˆæ¸…ç©ºReduxçŠ¶æ€
        dispatch(setAvailableProviders([]));
        dispatch(setSelectedProvider(''));
        dispatch(setSelectedModel(''));
        
        // ä½¿ç”¨ConfigManagerçš„æŒ‡ä»¤æ¨¡å‹åŠ è½½é…ç½®
        const configs = await configManager.loadConfigs();
        console.log('ğŸ”„ ChatPanel: ä»åç«¯è·å–åˆ°çš„åŸå§‹é…ç½®:', configs);
```

#### 3.1.2 ConfigManageræŒ‡ä»¤å¤„ç†

**æ–‡ä»¶**: `avatar-react/src/services/ConfigManager.ts`

```typescript
class ConfigManager {
  public cache = new Map<string, any>(); // æœ¬åœ°ç¼“å­˜ï¼ŒéæŒä¹…åŒ–
  private syncInterval = 30000; // 30ç§’åŒæ­¥ä¸€æ¬¡

  async loadConfigs(): Promise<Record<string, ProviderConfig>> {
    const response = await api.post('/config/command', {
      type: 'LOAD',
      timestamp: new Date().toISOString(),
      requestId: this.generateRequestId()
    });
    
    if (response.data.success) {
      this.cache.set('providers', response.data.data.providers);
      return response.data.data.providers;
    }
    
    throw new Error(response.data.message || 'åŠ è½½é…ç½®å¤±è´¥');
  }

  async syncConfigs(): Promise<Record<string, ProviderConfig>> {
    const response = await api.post('/config/command', {
      type: 'SYNC',
      timestamp: new Date().toISOString(),
      requestId: this.generateRequestId()
    });
    
    if (response.data.success) {
      this.cache.set('providers', response.data.data.providers);
      return response.data.data.providers;
    }
    
    return this.cache.get('providers') || {};
  }
}
```

### 3.2 å¯ç”¨æ¨¡å‹æ£€æµ‹

#### 3.2.1 æä¾›å•†çŠ¶æ€è½¬æ¢

```typescript
// è½¬æ¢é…ç½®ä¸ºå¯ç”¨æä¾›å•†åˆ—è¡¨
const enabledProviders = Object.entries(configs)
  .filter(([key, config]) => {
    const isEnabled = config.enabled && config.apiKey;
    console.log(`ğŸ” ChatPanel: æ£€æŸ¥æä¾›å•† ${key}:`, {
      enabled: config.enabled,
      hasApiKey: !!config.apiKey,
      isEnabled
    });
    return isEnabled;
  })
  .map(([key, config]) => ({
    key,
    name: config.name || key,
    models: config.enabledModels || [config.defaultModel].filter(Boolean),
    defaultModel: config.defaultModel,
    openaiCompatible: config.openaiCompatible || false
  }));

console.log('âœ… ChatPanel: å¤„ç†åçš„å¯ç”¨æä¾›å•†:', enabledProviders);
dispatch(setAvailableProviders(enabledProviders));
```

#### 3.2.2 æ¨¡å‹åˆ—è¡¨æ›´æ–°

```typescript
const loadModelsForProvider = async (providerKey: string) => {
  if (!providerKey) {
    setAvailableModels([]);
    return;
  }

  setLoadingModels(true);
  try {
    const provider = availableProviders.find(p => p.key === providerKey);
    if (provider && provider.models) {
      console.log(`ğŸ” ChatPanel: åŠ è½½æä¾›å•† ${providerKey} çš„æ¨¡å‹:`, provider.models);
      setAvailableModels(provider.models);
      
      // è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹
      if (provider.models.length > 0 && !selectedModel) {
        const defaultModel = provider.defaultModel || provider.models[0];
        console.log(`ğŸ¯ ChatPanel: è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹: ${defaultModel}`);
        dispatch(setSelectedModel(defaultModel));
      }
    }
  } catch (error) {
    console.error('âŒ ChatPanel: åŠ è½½æ¨¡å‹å¤±è´¥:', error);
    setAvailableModels([]);
    dispatch(setSelectedModel(''));
  } finally {
    setLoadingModels(false);
  }
};
```

### 3.3 SingleChatModelSelectorç»„ä»¶

**æ–‡ä»¶**: `avatar-react/src/components/chat/ModelSelectionDialog.tsx`

```typescript
const ModelSelectionDialog: React.FC<ModelSelectionProps> = ({ value, onChange, availableProviders, availableModels, onProviderChange }) => {
  const [selectedProvider, setSelectedProvider] = useState('');

  const handleProviderChange = (event) => {
    const providerKey = event.target.value;
    setSelectedProvider(providerKey);
    onProviderChange(providerKey);
    onChange(''); // æ¸…ç©ºæ¨¡å‹é€‰æ‹©
  };

  const handleModelChange = (event) => {
    onChange(event.target.value);
  };

  return (
    <Box sx={{ display: 'flex', gap: 2, mb: 2 }}>
      {/* æä¾›å•†é€‰æ‹©å™¨ */}
      <FormControl size="small" sx={{ minWidth: 150 }}>
        <InputLabel>AIæä¾›å•†</InputLabel>
        <Select
          value={selectedProvider}
          onChange={handleProviderChange}
          label="AIæä¾›å•†"
        >
          {availableProviders.map((provider) => (
            <MenuItem key={provider.key} value={provider.key}>
              <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
                <Typography>{provider.name}</Typography>
                <Chip 
                  label="å¯ç”¨" 
                  color="success" 
                  size="small" 
                  sx={{
                    height: 18,
                    fontSize: '0.65rem',
                    fontWeight: 'bold'
                  }}
                />
              </Box>
            </MenuItem>
          ))}
        </Select>
      </FormControl>

      {/* æ¨¡å‹é€‰æ‹©å™¨ */}
      <FormControl size="small" sx={{ minWidth: 200 }}>
        <InputLabel>æ¨¡å‹</InputLabel>
        <Select
          value={value}
          onChange={handleModelChange}
          label="æ¨¡å‹"
          disabled={!selectedProvider || availableModels.length === 0}
        >
          {availableModels.map((model) => (
            <MenuItem key={model} value={model}>
              <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
                <Typography>{model}</Typography>
                <Chip 
                  label="å¯ç”¨" 
                  color="success" 
                  size="small" 
                  sx={{
                    height: 18,
                    fontSize: '0.65rem',
                    fontWeight: 'bold'
                  }}
                />
              </Box>
            </MenuItem>
          ))}
        </Select>
      </FormControl>
    </Box>
  );
};
```

---

## æ¶ˆæ¯å‘é€æµç¨‹

### 4.1 å‰ç«¯æ¶ˆæ¯å¤„ç†

#### 4.1.1 æ¶ˆæ¯å‘é€å‡½æ•°

```typescript
const handleSendMessage = async () => {
  if (!inputMessage.trim() || isLoading) return;

  const messageContent = inputMessage.trim();
  setInputMessage('');

  try {
    // å…ˆæ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°æœ¬åœ°çŠ¶æ€
    dispatch(sendMessageAction({
      content: messageContent,
      role: 'user'
    }));

    // æ„å»ºæ¶ˆæ¯å¯¹è±¡å‘é€åˆ°åç«¯
    const messageData = {
      content: messageContent,
      role: 'user',
      provider: selectedProvider,
      model: selectedModel,
      chatMode: chatMode,
      ...(chatMode === 'group' && {
        groupSettings: groupChatSettings
      })
    };

    console.log('ğŸš€ ChatPanel: å‘é€æ¶ˆæ¯åˆ°åç«¯:', messageData);

    // å‘é€æ¶ˆæ¯åˆ°åç«¯
    const result = await dispatch(sendMessageThunk(messageData));
```

#### 4.1.2 Redux Thunkå¤„ç†

**æ–‡ä»¶**: `avatar-react/src/store/chatSlice.ts`

```typescript
export const sendMessage = createAsyncThunk(
  'chat/sendMessage',
  async (messageData: any, { rejectWithValue }) => {
    try {
      console.log('ğŸš€ sendMessage thunk: å‘é€æ¶ˆæ¯', messageData);
      
      const response = await chatAPI.sendMessage({
        message: messageData,
        provider: messageData.provider,
        model: messageData.model,
        chatMode: messageData.chatMode,
        ...(messageData.chatMode === 'group' && {
          models: messageData.groupSettings?.selectedModels || []
        })
      });

      console.log('âœ… sendMessage thunk: æ”¶åˆ°å“åº”', response);
      return response;
    } catch (error: any) {
      console.error('âŒ sendMessage thunk: å‘é€å¤±è´¥', error);
      return rejectWithValue(error.response?.data || error.message);
    }
  }
);
```

#### 4.1.3 APIæœåŠ¡å±‚

**æ–‡ä»¶**: `avatar-react/src/services/api.js`

```javascript
export const chatAPI = {
  async sendMessage(data) {
    console.log('ğŸŒ API: å‘é€èŠå¤©æ¶ˆæ¯', data);
    
    const response = await api.post('/chat/message', data);
    
    console.log('ğŸŒ API: æ”¶åˆ°èŠå¤©å“åº”', response.data);
    return response.data;
  },

  async getProviders() {
    const response = await api.get('/providers');
    return response.data;
  },

  async getModels(provider) {
    const response = await api.get(`/providers/${provider}/models`);
    return response.data;
  }
};
```

---

## åç«¯å¤„ç†æœºåˆ¶

### 5.1 æ¶ˆæ¯è·¯ç”±å¤„ç†

**æ–‡ä»¶**: `api-server/fastapi_stream.py`

#### 5.1.1 è·¯ç”±å®šä¹‰

```python
@app.post("/chat/message")
async def handle_chat_message(request: dict):
    """å¤„ç†èŠå¤©æ¶ˆæ¯çš„ä¸»è¦å…¥å£ç‚¹"""
    try:
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: {request}")
        
        # è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
        result = await chat_message(request)
        
        logger.info(f"èŠå¤©å¤„ç†å®Œæˆ: {type(result)}")
        return result
        
    except Exception as e:
        logger.error(f"èŠå¤©æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return {"error": f"èŠå¤©æ¶ˆæ¯å¤„ç†å¤±è´¥: {str(e)}"}
```

#### 5.1.2 æ ¸å¿ƒå¤„ç†å‡½æ•°

```python
async def chat_message(request: dict):
    """å¤„ç†èŠå¤©æ¶ˆæ¯ - ç®€åŒ–ç‰ˆæœ¬"""
    try:
        import time
        start_time = time.time()
        first_token_time = None
        
        # å¤„ç†å‰ç«¯å‘é€çš„æ¶ˆæ¯æ ¼å¼
        message_data = request.get("message", {})
        if isinstance(message_data, dict):
            message = message_data.get("content", "")
            provider = message_data.get("provider") or request.get("provider", "deepseek")
            model = message_data.get("model") or request.get("model", "deepseek-chat")
        else:
            # å…¼å®¹æ—§æ ¼å¼
            message = str(message_data) if message_data else ""
            provider = request.get("provider", "deepseek")
            model = request.get("model", "deepseek-chat")
        
        models = request.get("models", [model])  # ç¾¤èŠæ”¯æŒå¤šæ¨¡å‹
        
        logger.info(f"æ”¶åˆ°èŠå¤©æ¶ˆæ¯: {message[:50] if len(message) > 50 else message}..., provider: {provider}, model: {model}")
```

### 5.2 é…ç½®éªŒè¯

```python
# éªŒè¯æä¾›å•†é…ç½®
from config_manager import config_manager
provider_config = config_manager.get_provider_config(provider)

if not provider_config or not provider_config.get('enabled') or not provider_config.get('api_key'):
    return {"error": f"æä¾›å•† {provider} æœªé…ç½®æˆ–æœªå¯ç”¨"}
```

---

## æä¾›å•†å…¼å®¹æ€§åˆ¤æ–­

### 6.1 OpenAIå…¼å®¹æ¨¡å¼æ£€æµ‹

```python
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
openai_compatible = provider_config.get('openaiCompatible', False)

# æ ¹æ®å…¼å®¹æ¨¡å¼é€‰æ‹©æä¾›å•†å®ä¾‹
if openai_compatible and provider != 'openai':
    # ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
    from providers.openai import OpenAIProvider
    from providers.base import ProviderConfig, ProviderType
    
    temp_config = ProviderConfig(
        provider_type=ProviderType.OPENAI,
        api_key=provider_config.get('api_key', ''),
        base_url=provider_config.get('baseUrl', ''),
        default_model=provider_config.get('defaultModel', model)
    )
    provider_instance = OpenAIProvider(temp_config)
else:
    # ä½¿ç”¨å®˜æ–¹SDKæˆ–å·²æ³¨å†Œçš„æä¾›å•†
    from providers.manager import ProviderManager
    provider_manager = ProviderManager()
    
    provider_instance = provider_manager.get_provider(provider)
    if not provider_instance:
        # å¦‚æœæ˜¯å®˜æ–¹SDKæ¨¡å¼ä½†æœªå®ç°ï¼Œè¿”å›é”™è¯¯
        if not openai_compatible:
            return {"error": f"æä¾›å•† {provider} å®˜æ–¹SDKæ¨¡å¼æ­£åœ¨å¼€å‘ä¸­ï¼Œè¯·å¯ç”¨OpenAIå…¼å®¹æ¨¡å¼"}
        else:
            return {"error": f"æä¾›å•† {provider} ä¸å¯ç”¨"}
```

### 6.2 æä¾›å•†å®ä¾‹åˆ›å»º

#### 6.2.1 OpenAIæä¾›å•†

**æ–‡ä»¶**: `api-server/providers/openai.py`

```python
class OpenAIProvider(BaseProvider):
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            base_url=config.base_url or "https://api.openai.com/v1"
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        stream: bool = True,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> AsyncGenerator[StreamChunk, None]:
        """æ‰§è¡ŒèŠå¤©å®Œæˆè¯·æ±‚"""
        request_id = str(uuid.uuid4())[:8]
        start_time = time.time()
        
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            converted_messages = self._convert_messages(messages)
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": model,
                "messages": converted_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": stream
            }
            
            if not stream:
                # éæµå¼å“åº”
                response = await self.client.chat.completions.create(**request_params)
                
                if response.choices and len(response.choices) > 0:
                    content = response.choices[0].message.content
                    
                    # æå–usageä¿¡æ¯
                    usage_info = None
                    if hasattr(response, 'usage') and response.usage:
                        usage_info = {
                            "prompt_tokens": response.usage.prompt_tokens,
                            "completion_tokens": response.usage.completion_tokens,
                            "total_tokens": response.usage.total_tokens
                        }
                    
                    yield StreamChunk(
                        content=content,
                        chunk_id=1,
                        request_id=request_id,
                        timestamp=time.time(),
                        model=model,
                        provider=self.provider_name,
                        usage=usage_info
                    )
```

#### 6.2.2 GLMæä¾›å•†

**æ–‡ä»¶**: `api-server/providers/glm.py`

```python
class GLMProvider(BaseProvider):
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            base_url=config.base_url or "https://open.bigmodel.cn/api/paas/v4"
        )

    async def chat_completion(self, messages, model, stream=False, **kwargs):
        """GLMèŠå¤©å®Œæˆå®ç°"""
        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                stream=stream,
                **kwargs
            )
            
            if not stream:
                content = response.choices[0].message.content
                
                # æå–usageä¿¡æ¯
                usage_info = None
                if hasattr(response, 'usage') and response.usage:
                    usage_info = {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                
                yield StreamChunk(
                    content=content,
                    chunk_id=1,
                    request_id=str(uuid.uuid4())[:8],
                    timestamp=time.time(),
                    model=model,
                    provider=self.provider_name,
                    usage=usage_info
                )
```

---

## Tokenç»Ÿè®¡ä¸è´¹ç”¨è®¡ç®—

### 7.1 Tokenä½¿ç”¨ä¿¡æ¯æå–

#### 7.1.1 å•èŠæ¨¡å¼å¤„ç†

```python
# å•èŠæ¨¡å¼ï¼šä½¿ç”¨å•ä¸ªæ¨¡å‹
selected_model = model

async for chunk in provider_instance.chat_completion(
    messages=[{"role": "user", "content": message}],
    model=selected_model,
    stream=False
):
    if first_token_time is None:
        first_token_time = time.time() - start_time
    
    if hasattr(chunk, 'content'):
        response_content += chunk.content
        chunk_count += 1
        # æå–tokenä½¿ç”¨ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªchunkæˆ–CompletionResponseä¸­ï¼‰
        if hasattr(chunk, 'usage') and chunk.usage:
            usage_info = chunk.usage
    elif isinstance(chunk, str):
        response_content += chunk
        chunk_count += 1
```

#### 7.1.2 çœŸå®Token vs ä¼°ç®—Token

```python
# ä½¿ç”¨çœŸå®tokenä½¿ç”¨ä¿¡æ¯æˆ–ä¼°ç®—
if usage_info:
    # ä½¿ç”¨ä»APIå“åº”ä¸­è·å–çš„çœŸå®tokenä½¿ç”¨ä¿¡æ¯
    input_tokens = usage_info.get('prompt_tokens', 0)
    output_tokens = usage_info.get('completion_tokens', 0)
    total_tokens = usage_info.get('total_tokens', input_tokens + output_tokens)
    cache_tokens = usage_info.get('cache_creation_input_tokens', 0) + usage_info.get('cache_read_input_tokens', 0)
else:
    # å›é€€åˆ°ä¼°ç®—tokenæ•°é‡
    input_tokens = len(message) // 2 if any('\u4e00' <= c <= '\u9fff' for c in message) else len(message) // 4
    input_tokens = max(10, input_tokens)
    
    output_tokens = len(response_content) // 2 if any('\u4e00' <= c <= '\u9fff' for c in response_content) else len(response_content) // 4
    output_tokens = max(1, output_tokens)
    
    total_tokens = input_tokens + output_tokens
    cache_tokens = 0

tokens_per_second = output_tokens / total_time if total_time > 0 else 0
```

### 7.2 å¤šæä¾›å•†è´¹ç”¨è®¡ç®—

#### 7.2.1 åŠ¨æ€è´¹ç”¨è®¡ç®—é€»è¾‘

```python
# åŠ¨æ€è´¹ç”¨è®¡ç®—ï¼ˆæ ¹æ®ä¸åŒæä¾›å•†çš„è®¡è´¹æ¨¡å¼ï¼‰
# è·å–å½“å‰æ±‡ç‡
usd_to_cny_rate = get_current_usd_to_cny_rate()

# æ£€æŸ¥æ˜¯å¦æœ‰APIè¿”å›çš„å®é™…è´¹ç”¨ä¿¡æ¯ï¼ˆå¦‚OpenRouterï¼‰
if usage_info and 'cost' in usage_info:
    # ä½¿ç”¨APIè¿”å›çš„å®é™…è´¹ç”¨ï¼ˆé€šå¸¸æ˜¯ç¾å…ƒï¼‰
    api_cost_usd = usage_info.get('cost', 0)
    total_cost_cny = api_cost_usd * usd_to_cny_rate
    # ä¼°ç®—è¾“å…¥è¾“å‡ºè´¹ç”¨åˆ†é…ï¼ˆé€šå¸¸è¾“å‡ºè´¹ç”¨æ˜¯è¾“å…¥çš„3å€ï¼‰
    total_cost_usd = api_cost_usd
    input_cost = total_cost_usd * 0.25  # 25%è¾“å…¥è´¹ç”¨
    output_cost = total_cost_usd * 0.75  # 75%è¾“å‡ºè´¹ç”¨
else:
    # æŒ‰æä¾›å•†è®¡è´¹æ¨¡å¼è®¡ç®—
    if provider == 'deepseek':
        # DeepSeekæŒ‰äººæ°‘å¸è®¡è´¹ï¼Œæ— éœ€æ±‡ç‡è½¬æ¢
        cost_per_1k_input_cny = 0.0007  # 0.07åˆ†/1K tokens
        cost_per_1k_output_cny = 0.0014  # 0.14åˆ†/1K tokens
        input_cost = (input_tokens / 1000) * cost_per_1k_input_cny / usd_to_cny_rate  # è½¬æ¢ä¸ºç¾å…ƒç­‰ä»·
        output_cost = (output_tokens / 1000) * cost_per_1k_output_cny / usd_to_cny_rate
        total_cost_cny = (input_tokens / 1000) * cost_per_1k_input_cny + (output_tokens / 1000) * cost_per_1k_output_cny
    elif provider == 'glm':
        # GLMæŒ‰äººæ°‘å¸è®¡è´¹
        cost_per_1k_input_cny = 0.005  # 0.5åˆ†/1K tokens
        cost_per_1k_output_cny = 0.005  # 0.5åˆ†/1K tokens  
        input_cost = (input_tokens / 1000) * cost_per_1k_input_cny / usd_to_cny_rate
        output_cost = (output_tokens / 1000) * cost_per_1k_output_cny / usd_to_cny_rate
        total_cost_cny = (input_tokens / 1000) * cost_per_1k_input_cny + (output_tokens / 1000) * cost_per_1k_output_cny
    else:
        # å…¶ä»–æä¾›å•†æŒ‰ç¾å…ƒè®¡è´¹
        cost_per_1k_input = {
            'openai': 0.005,
            'anthropic': 0.003,
            'google': 0.00125,
            'openrouter': 0.005
        }.get(provider, 0.001)
        
        cost_per_1k_output = {
            'openai': 0.015,
            'anthropic': 0.015,
            'google': 0.005,
            'openrouter': 0.015
        }.get(provider, 0.002)
        
        input_cost = (input_tokens / 1000) * cost_per_1k_input
        output_cost = (output_tokens / 1000) * cost_per_1k_output
        total_cost_cny = (input_cost + output_cost) * usd_to_cny_rate
```

#### 7.2.2 æ±‡ç‡æœåŠ¡

**æ–‡ä»¶**: `api-server/utils/exchange_rate.py`

```python
class ExchangeRateService:
    def __init__(self):
        self._usd_to_cny_rate = 7.2  # é»˜è®¤æ±‡ç‡
        self._last_update_time = 0
        self._update_interval = 3600  # 1å°æ—¶æ›´æ–°ä¸€æ¬¡
        
    def _initialize_rate_sync(self):
        """åŒæ­¥åˆå§‹åŒ–æ±‡ç‡ï¼ˆä»…ä»ç¼“å­˜è¯»å–ï¼‰"""
        try:
            # å°è¯•ä»ç¼“å­˜æ–‡ä»¶è¯»å–
            try:
                with open('cache/exchange_rate.json', 'r') as f:
                    data = json.load(f)
                    cached_rate = data.get('rate')
                    cached_time = data.get('timestamp', 0)
                    
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    if cached_rate and (time.time() - cached_time) < self._update_interval:
                        self._usd_to_cny_rate = cached_rate
                        self._last_update_time = cached_time
                        logger.info(f"ä»ç¼“å­˜åŠ è½½æ±‡ç‡: 1 USD = {self._usd_to_cny_rate} CNY")
                        return
            except (FileNotFoundError, json.JSONDecodeError):
                pass
            
            logger.info(f"ä½¿ç”¨é»˜è®¤æ±‡ç‡: 1 USD = {self._usd_to_cny_rate} CNY")
            
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–æ±‡ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ±‡ç‡ {self._usd_to_cny_rate}: {e}")

    async def _fetch_latest_rate(self) -> None:
        """è·å–æœ€æ–°æ±‡ç‡"""
        apis = [
            'https://api.exchangerate-api.com/v4/latest/USD',
            'https://open.er-api.com/v6/latest/USD',
            'https://api.fxratesapi.com/latest?base=USD&symbols=CNY'
        ]
        
        for api_url in apis:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(api_url, timeout=5) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            # ä¸åŒAPIçš„å“åº”æ ¼å¼å¤„ç†
                            if 'rates' in data and 'CNY' in data['rates']:
                                new_rate = float(data['rates']['CNY'])
                            elif 'CNY' in data:
                                new_rate = float(data['CNY'])
                            else:
                                continue
                            
                            # éªŒè¯æ±‡ç‡åˆç†æ€§
                            if 6.0 <= new_rate <= 8.0:
                                self._usd_to_cny_rate = new_rate
                                self._last_update_time = time.time()
                                
                                # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                                await self._save_to_cache(new_rate)
                                
                                logger.info(f"æ±‡ç‡æ›´æ–°æˆåŠŸ: 1 USD = {new_rate} CNY (æ¥æº: {api_url})")
                                return
                            
            except Exception as e:
                logger.warning(f"ä» {api_url} è·å–æ±‡ç‡å¤±è´¥: {e}")
                continue
        
        logger.warning("æ‰€æœ‰æ±‡ç‡APIéƒ½å¤±è´¥ï¼Œä¿æŒå½“å‰æ±‡ç‡")

def get_current_usd_to_cny_rate() -> float:
    """è·å–å½“å‰USDåˆ°CNYæ±‡ç‡"""
    return _exchange_rate_service.get_rate()
```

### 7.3 å“åº”æ•°æ®ç»“æ„

```python
return {
    "response": response_content,
    "provider": provider,
    "model": selected_model,
    "performance": {
        "first_token_time": first_token_time or 0,
        "response_time": total_time,
        "tokens_per_second": tokens_per_second
    },
    "tokens": {
        "input": input_tokens,
        "output": output_tokens,
        "cache": cache_tokens,
        "total": total_tokens,
        "prompt_tokens": input_tokens,  # ä¿æŒå‘åå…¼å®¹
        "completion_tokens": output_tokens,  # ä¿æŒå‘åå…¼å®¹
        "total_tokens": total_tokens,  # ä¿æŒå‘åå…¼å®¹
        "input_cost": input_cost,
        "output_cost": output_cost,
        "cache_cost": (cache_tokens / 1000) * (input_cost / input_tokens * 1000 if input_tokens > 0 else 0) * 0.5,
        "total_cost_cny": total_cost_cny
    }
}
```

---

## æµå¼å“åº”ä¸æ‰“å­—æœºæ•ˆæœ

### 8.1 å‰ç«¯å“åº”å¤„ç†

#### 8.1.1 å“åº”ç»“æœå¤„ç†

```typescript
// å‘é€æ¶ˆæ¯åˆ°åç«¯
const result = await dispatch(sendMessageThunk(messageData));

if (result.type && result.type.endsWith('/fulfilled')) {
  console.log('âœ… ChatPanel: æ¶ˆæ¯å‘é€æˆåŠŸ:', result.payload);
  
  // æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
  const response = result.payload as any;
  if (response && response.error) {
    console.error('âŒ ChatPanel: åç«¯è¿”å›é”™è¯¯:', response.error);
    // æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°èŠå¤©ç•Œé¢
    const errorMessage = response.error || 'æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œè¯·é‡è¯•';
    
    // å¤„ç†é•¿URLçš„æ¢è¡Œ
    const processedErrorMessage = errorMessage.replace(
      /(https?:\/\/[^\s,}]+)/g, 
      (url) => `\n${url}\n`
    );
    
    dispatch(sendMessageAction({
      id: `error_${Date.now()}`,
      content: `âŒ å‘é€å¤±è´¥ï¼š\n${processedErrorMessage}`,
      role: 'assistant',
      type: 'error'
    }));
    return;
  }
```

#### 8.1.2 æ‰“å­—æœºæ•ˆæœå®ç°

```typescript
// æ‰“å­—æœºæ•ˆæœå‡½æ•°
const typewriterEffect = (messageId: string, fullContent: string, speed: number = 30) => {
  setTypingMessageId(messageId);
  
  let index = 0;
  const timer = setInterval(() => {
    if (index < fullContent.length) {
      // ä½¿ç”¨Reduxçš„updateMessageæ¥æ›´æ–°æ¶ˆæ¯å†…å®¹
      dispatch(updateMessage({
        id: messageId,
        content: fullContent.substring(0, index + 1)
      }));
      index++;
      scrollToBottom();
    } else {
      clearInterval(timer);
      setTypingMessageId(null);
      // ç¡®ä¿æœ€ç»ˆå†…å®¹å®Œæ•´
      dispatch(updateMessage({
        id: messageId,
        content: fullContent
      }));
    }
  }, speed);
  
  return timer;
};
```

#### 8.1.3 AIæ¶ˆæ¯æ˜¾ç¤º

```typescript
// å¦‚æœæœ‰AIå›å¤ï¼Œå¯åŠ¨æ‰“å­—æœºæ•ˆæœ
if (response && typeof response === 'object' && 'response' in response) {
  // å•èŠæ¨¡å¼
  const aiMessageId = `ai_${Date.now()}`;
  const aiContent = String(response.response || '');
  
  // å…ˆæ·»åŠ ç©ºçš„AIæ¶ˆæ¯
  dispatch(sendMessageAction({
    id: aiMessageId,
    content: '',
    role: 'assistant',
    provider: response.provider || undefined,
    model: response.model || undefined,
    performance: response.performance || undefined,
    tokens: response.tokens || undefined
  }));
  
  // å¯åŠ¨æ‰“å­—æœºæ•ˆæœ
  setTimeout(() => {
    typewriterEffect(aiMessageId, aiContent);
  }, 100);
}
```

### 8.2 æ¶ˆæ¯æ˜¾ç¤ºç»„ä»¶

#### 8.2.1 MessageListç»„ä»¶

**æ–‡ä»¶**: `avatar-react/src/components/chat/MessageList.tsx`

```typescript
const MessageList: React.FC<MessageListProps> = ({ messages, typingMessageId, usdToCnyRate }) => {
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  return (
    <List sx={{ flexGrow: 1, overflow: 'auto', maxHeight: '60vh' }}>
      {messages.map((message, index) => (
        <ListItem key={message.id || index} sx={{ display: 'block', py: 1 }}>
          <Paper
            elevation={1}
            sx={{
              p: 2,
              backgroundColor: message.role === 'user' ? 'primary.light' : 'grey.100',
              color: message.role === 'user' ? 'primary.contrastText' : 'text.primary',
              ...(message.type === 'error' && {
                backgroundColor: 'error.light',
                color: 'error.contrastText'
              })
            }}
          >
            {/* æ¶ˆæ¯å¤´éƒ¨ä¿¡æ¯ */}
            <Box sx={{ display: 'flex', alignItems: 'center', mb: 1 }}>
              {message.role === 'user' ? (
                <PersonIcon sx={{ mr: 1 }} />
              ) : (
                <AIIcon sx={{ mr: 1 }} />
              )}
              <Typography variant="subtitle2">
                {message.role === 'user' ? 'ç”¨æˆ·' : 'AIåŠ©æ‰‹'}
                {message.provider && ` (${message.provider})`}
                {message.model && ` - ${message.model}`}
              </Typography>
            </Box>

            {/* æ¶ˆæ¯å†…å®¹ */}
            <MessageContent 
              content={message.content} 
              isTyping={typingMessageId === message.id}
            />

            {/* Tokenå’Œæ€§èƒ½ä¿¡æ¯ */}
            {message.tokens && (
              <TokenDisplay tokens={message.tokens} usdToCnyRate={usdToCnyRate} />
            )}
            
            {message.performance && (
              <PerformanceDisplay performance={message.performance} />
            )}
          </Paper>
        </ListItem>
      ))}
      <div ref={messagesEndRef} />
    </List>
  );
};
```

#### 8.2.2 Tokenæ˜¾ç¤ºç»„ä»¶

```typescript
const TokenDisplay: React.FC<{ tokens: any; usdToCnyRate: number }> = ({ tokens, usdToCnyRate }) => {
  return (
    <Box sx={{ mt: 1, p: 1, backgroundColor: 'rgba(0,0,0,0.05)', borderRadius: 1 }}>
      <Typography variant="caption" sx={{ display: 'block', mb: 0.5 }}>
        ğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡
      </Typography>
      
      <Box sx={{ display: 'flex', gap: 2, flexWrap: 'wrap' }}>
        <Typography variant="caption">
          è¾“å…¥: {tokens.input || tokens.prompt_tokens || 0} tokens
        </Typography>
        <Typography variant="caption">
          è¾“å‡º: {tokens.output || tokens.completion_tokens || 0} tokens
        </Typography>
        {tokens.cache > 0 && (
          <Typography variant="caption">
            ç¼“å­˜: {tokens.cache} tokens
          </Typography>
        )}
        <Typography variant="caption">
          æ€»è®¡: {tokens.total || tokens.total_tokens || 0} tokens
        </Typography>
      </Box>
      
      {tokens.total_cost_cny && (
        <Box sx={{ mt: 0.5 }}>
          <Typography variant="caption" sx={{ display: 'block' }}>
            ğŸ’° è´¹ç”¨è¯¦æƒ…
          </Typography>
          <Box sx={{ display: 'flex', gap: 2, flexWrap: 'wrap' }}>
            <Typography variant="caption">
              è¾“å…¥è´¹ç”¨: Â¥{(tokens.input_cost * usdToCnyRate).toFixed(4)}
            </Typography>
            <Typography variant="caption">
              è¾“å‡ºè´¹ç”¨: Â¥{(tokens.output_cost * usdToCnyRate).toFixed(4)}
            </Typography>
            <Typography variant="caption" sx={{ fontWeight: 'bold' }}>
              æ€»è´¹ç”¨: Â¥{tokens.total_cost_cny.toFixed(4)}
            </Typography>
          </Box>
        </Box>
      )}
    </Box>
  );
};
```

---

## é”™è¯¯å¤„ç†æœºåˆ¶

### 9.1 å‰ç«¯é”™è¯¯å¤„ç†

#### 9.1.1 ç½‘ç»œé”™è¯¯å¤„ç†

```typescript
} else {
  console.error('âŒ ChatPanel: æ¶ˆæ¯å‘é€å¤±è´¥:', result.payload);
  // æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°èŠå¤©ç•Œé¢
  dispatch(sendMessageAction({
    id: `error_${Date.now()}`,
    content: `âŒ å‘é€å¤±è´¥ï¼š${result.payload || 'æœªçŸ¥é”™è¯¯ï¼Œè¯·é‡è¯•'}`,
    role: 'assistant',
    type: 'error'
  }));
}
```

#### 9.1.2 é…ç½®é”™è¯¯å¤„ç†

```typescript
} catch (error) {
  console.error('âŒ ChatPanel: åŠ è½½æä¾›å•†é…ç½®å¤±è´¥:', error);
  dispatch(setProvidersLoading(false));
  
  // æ˜¾ç¤ºé”™è¯¯æç¤º
  dispatch(sendMessageAction({
    id: `config_error_${Date.now()}`,
    content: `âŒ é…ç½®åŠ è½½å¤±è´¥ï¼š${error.message || 'è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œåç«¯æœåŠ¡'}`,
    role: 'assistant',
    type: 'error'
  }));
}
```

### 9.2 åç«¯é”™è¯¯å¤„ç†

#### 9.2.1 æä¾›å•†é”™è¯¯å¤„ç†

```python
# éªŒè¯æä¾›å•†é…ç½®
from config_manager import config_manager
provider_config = config_manager.get_provider_config(provider)

if not provider_config or not provider_config.get('enabled') or not provider_config.get('api_key'):
    return {"error": f"æä¾›å•† {provider} æœªé…ç½®æˆ–æœªå¯ç”¨"}

# æ£€æŸ¥æä¾›å•†å®ä¾‹
provider_instance = provider_manager.get_provider(provider)
if not provider_instance:
    if not openai_compatible:
        return {"error": f"æä¾›å•† {provider} å®˜æ–¹SDKæ¨¡å¼æ­£åœ¨å¼€å‘ä¸­ï¼Œè¯·å¯ç”¨OpenAIå…¼å®¹æ¨¡å¼"}
    else:
        return {"error": f"æä¾›å•† {provider} ä¸å¯ç”¨"}
```

#### 9.2.2 APIè°ƒç”¨é”™è¯¯å¤„ç†

```python
try:
    async for chunk in provider_instance.chat_completion(
        messages=[{"role": "user", "content": message}],
        model=selected_model,
        stream=False
    ):
        # å¤„ç†å“åº”...
        
except Exception as e:
    logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
    return {"error": f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"}
```

#### 9.2.3 å…¨å±€å¼‚å¸¸å¤„ç†

```python
except Exception as e:
    logger.error(f"èŠå¤©æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
    return {"error": f"èŠå¤©æ¶ˆæ¯å¤„ç†å¤±è´¥: {str(e)}"}
```

---

## æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

### 10.1 æ€§èƒ½æŒ‡æ ‡æ”¶é›†

#### 10.1.1 å“åº”æ—¶é—´ç»Ÿè®¡

```python
# è®¡ç®—å¼€å§‹æ—¶é—´ç”¨äºæ€§èƒ½ç»Ÿè®¡
start_time = time.time()

# é¦–å­—å»¶è¿Ÿç»Ÿè®¡
if first_token_time is None:
    first_token_time = time.time() - start_time

# è®¡ç®—æ€§èƒ½ç»Ÿè®¡
total_time = time.time() - start_time
tokens_per_second = output_tokens / total_time if total_time > 0 else 0

"performance": {
    "first_token_time": first_token_time or 0,
    "response_time": total_time,
    "tokens_per_second": tokens_per_second
}
```

#### 10.1.2 å‰ç«¯æ€§èƒ½æ˜¾ç¤º

```typescript
const PerformanceDisplay: React.FC<{ performance: any }> = ({ performance }) => {
  return (
    <Box sx={{ mt: 1, p: 1, backgroundColor: 'rgba(0,0,0,0.05)', borderRadius: 1 }}>
      <Typography variant="caption" sx={{ display: 'block', mb: 0.5 }}>
        âš¡ æ€§èƒ½æŒ‡æ ‡
      </Typography>
      
      <Box sx={{ display: 'flex', gap: 2, flexWrap: 'wrap' }}>
        <Typography variant="caption">
          é¦–å­—å»¶è¿Ÿ: {(performance.first_token_time * 1000).toFixed(0)}ms
        </Typography>
        <Typography variant="caption">
          æ€»å“åº”æ—¶é—´: {performance.response_time.toFixed(2)}s
        </Typography>
        <Typography variant="caption">
          ç”Ÿæˆé€Ÿåº¦: {performance.tokens_per_second.toFixed(1)} tokens/s
        </Typography>
      </Box>
    </Box>
  );
};
```

### 10.2 ç¼“å­˜ä¼˜åŒ–

#### 10.2.1 é…ç½®ç¼“å­˜

```typescript
class ConfigManager {
  public cache = new Map<string, any>(); // æœ¬åœ°ç¼“å­˜ï¼ŒéæŒä¹…åŒ–
  private syncInterval = 30000; // 30ç§’åŒæ­¥ä¸€æ¬¡

  constructor() {
    this.startPeriodicSync(); // å¯åŠ¨å®šæœŸåŒæ­¥
  }

  private startPeriodicSync(): void {
    setInterval(async () => {
      try {
        await this.syncConfigs();
      } catch (error) {
        console.warn('å®šæœŸåŒæ­¥é…ç½®å¤±è´¥:', error);
      }
    }, this.syncInterval);
  }
}
```

#### 10.2.2 æ±‡ç‡ç¼“å­˜

```typescript
// æ±‡ç‡ç¼“å­˜ç­–ç•¥
const cached = localStorage.getItem('usd_to_cny_rate');
if (cached) {
  const { rate, timestamp } = JSON.parse(cached);
  // ç¼“å­˜æœ‰æ•ˆæœŸ24å°æ—¶
  if (Date.now() - timestamp < 24 * 60 * 60 * 1000) {
    return rate;
  }
}
```

### 10.3 å†…å­˜ç®¡ç†

#### 10.3.1 æ¶ˆæ¯å†å²é™åˆ¶

```typescript
// é™åˆ¶æ¶ˆæ¯å†å²é•¿åº¦ï¼Œé¿å…å†…å­˜æ³„æ¼
const MAX_MESSAGES = 100;

const addMessage = (state, action) => {
  state.messages.push(action.payload);
  
  // ä¿æŒæ¶ˆæ¯æ•°é‡åœ¨é™åˆ¶å†…
  if (state.messages.length > MAX_MESSAGES) {
    state.messages = state.messages.slice(-MAX_MESSAGES);
  }
};
```

#### 10.3.2 å®šæ—¶å™¨æ¸…ç†

```typescript
useEffect(() => {
  const interval = setInterval(initExchangeRate, 60 * 60 * 1000);
  
  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†å®šæ—¶å™¨
  return () => clearInterval(interval);
}, []);
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†AIå•èŠåŠŸèƒ½çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š

1. **ç»„ä»¶åˆå§‹åŒ–**ï¼šä»é…ç½®åŠ è½½åˆ°æ¨¡å‹æ£€æµ‹çš„å®Œæ•´æµç¨‹
2. **æ¶ˆæ¯å¤„ç†**ï¼šå‰åç«¯åä½œçš„æ¶ˆæ¯å‘é€å’Œå“åº”æœºåˆ¶
3. **æä¾›å•†ç®¡ç†**ï¼šå¤šæä¾›å•†å…¼å®¹æ€§å’ŒåŠ¨æ€åˆ‡æ¢
4. **Tokenç»Ÿè®¡**ï¼šçœŸå®APIæ•°æ®æå–å’Œå¤šå¸ç§è´¹ç”¨è®¡ç®—
5. **ç”¨æˆ·ä½“éªŒ**ï¼šæ‰“å­—æœºæ•ˆæœå’Œå®æ—¶åé¦ˆ
6. **é”™è¯¯å¤„ç†**ï¼šå…¨é“¾è·¯é”™è¯¯æ•è·å’Œç”¨æˆ·å‹å¥½æç¤º
7. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜ç­–ç•¥å’Œèµ„æºç®¡ç†

é€šè¿‡ç†è§£è¿™äº›å®ç°ç»†èŠ‚ï¼Œå¼€å‘è€…å¯ä»¥ï¼š
- å®Œå…¨æŒæ¡å•èŠåŠŸèƒ½çš„å·¥ä½œåŸç†
- æ‰©å±•æ”¯æŒæ–°çš„AIæä¾›å•†
- ä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ
- å®ç°ç±»ä¼¼çš„èŠå¤©åŠŸèƒ½

è¯¥æ¶æ„å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œç»´æŠ¤æ€§ï¼Œä¸ºåç»­åŠŸèƒ½å¼€å‘æä¾›äº†åšå®çš„åŸºç¡€ã€‚