# 群聊功能深度解析文档

## 概述

群聊功能是基于WebSocket实现的多模型实时对话系统，允许用户同时与多个AI大模型进行群组对话。该功能采用"最先收到消息最先回复"的机制，所有大模型共享同一个上下文（聊天记录），并支持个性化系统提示词配置。

## 核心特性

### 1. 实时通信机制
- **WebSocket连接**：基于WebSocket实现实时双向通信
- **消息优先级**：最先收到用户消息的大模型最先回复
- **并发处理**：支持多个大模型同时处理和回复消息

### 2. 上下文管理
- **共享上下文**：所有大模型共享同一个聊天记录上下文
- **实时显示**：聊天框下方实时显示当前上下文大小
- **个性化显示**：每个大模型回复时单独显示其上下文空间剩余

### 3. 大模型选择与配置
- **自动调用信息卡**：切换到群聊时自动弹出大模型选择界面
- **灵活配置**：支持个性化和统一两种系统提示词配置模式
- **动态管理**：支持群聊过程中动态添加或移除大模型

## 技术架构

### 前端架构

```
GroupChatPage (群聊页面)
├── ModelSelectionDialog (大模型选择对话框)
│   ├── InfoCard (信息卡组件)
│   └── SystemPromptConfig (系统提示词配置)
├── GroupChatInterface (群聊界面)
│   ├── MessageList (消息列表)
│   ├── ContextDisplay (上下文显示)
│   ├── ModelStatusBar (模型状态栏)
│   └── MessageInput (消息输入框)
└── WebSocketManager (WebSocket管理器)
```

### 后端架构

```
WebSocket Handler
├── ConnectionManager (连接管理器)
├── MessageRouter (消息路由器)
├── ContextManager (上下文管理器)
├── ModelOrchestrator (模型编排器)
└── ResponseAggregator (响应聚合器)
```

## 详细实现方案

### 1. 前端实现

#### 1.1 群聊页面组件 (GroupChatPage.jsx)

```jsx
import React, { useState, useEffect, useRef } from 'react';
import { 
  Box, 
  Container, 
  Paper, 
  Typography, 
  Chip,
  LinearProgress,
  Alert
} from '@mui/material';
import ModelSelectionDialog from '../components/ModelSelectionDialog';
import GroupChatInterface from '../components/GroupChatInterface';
import WebSocketManager from '../services/WebSocketManager';

const GroupChatPage = () => {
  // 状态管理
  const [selectedModels, setSelectedModels] = useState([]);
  const [messages, setMessages] = useState([]);
  const [contextSize, setContextSize] = useState(0);
  const [modelStatuses, setModelStatuses] = useState({});
  const [isSelectionOpen, setIsSelectionOpen] = useState(true);
  const [wsConnected, setWsConnected] = useState(false);
  
  // WebSocket管理器引用
  const wsManager = useRef(null);

  // 组件挂载时初始化
  useEffect(() => {
    // 自动打开模型选择对话框
    setIsSelectionOpen(true);
    
    return () => {
      // 清理WebSocket连接
      if (wsManager.current) {
        wsManager.current.disconnect();
      }
    };
  }, []);

  // 处理模型选择完成
  const handleModelSelection = async (models, systemPrompts) => {
    setSelectedModels(models);
    setIsSelectionOpen(false);
    
    // 初始化WebSocket连接
    wsManager.current = new WebSocketManager({
      onConnect: () => setWsConnected(true),
      onDisconnect: () => setWsConnected(false),
      onMessage: handleWebSocketMessage,
      onError: handleWebSocketError
    });
    
    // 连接并配置群聊
    await wsManager.current.connect();
    await wsManager.current.initializeGroupChat(models, systemPrompts);
  };

  // 处理WebSocket消息
  const handleWebSocketMessage = (message) => {
    switch (message.type) {
      case 'model_response':
        setMessages(prev => [...prev, {
          id: Date.now(),
          type: 'ai',
          content: message.content,
          modelId: message.modelId,
          modelName: message.modelName,
          timestamp: new Date()
        }]);
        break;
        
      case 'context_update':
        setContextSize(message.contextSize);
        break;
        
      case 'model_status':
        setModelStatuses(prev => ({
          ...prev,
          [message.modelId]: message.status
        }));
        break;
        
      default:
        console.log('Unknown message type:', message.type);
    }
  };

  // 处理WebSocket错误
  const handleWebSocketError = (error) => {
    console.error('WebSocket error:', error);
    // 显示错误提示
  };

  // 发送消息
  const handleSendMessage = async (content) => {
    if (!wsManager.current || !wsConnected) {
      console.error('WebSocket not connected');
      return;
    }

    // 添加用户消息到界面
    const userMessage = {
      id: Date.now(),
      type: 'user',
      content,
      timestamp: new Date()
    };
    setMessages(prev => [...prev, userMessage]);

    // 通过WebSocket发送消息
    await wsManager.current.sendMessage(content);
  };

  return (
    <Container maxWidth="lg" sx={{ py: 2 }}>
      {/* 模型选择对话框 */}
      <ModelSelectionDialog
        open={isSelectionOpen}
        onClose={() => setIsSelectionOpen(false)}
        onConfirm={handleModelSelection}
      />

      {/* 主界面 */}
      {!isSelectionOpen && (
        <Paper elevation={3} sx={{ height: 'calc(100vh - 100px)' }}>
          {/* 连接状态指示器 */}
          {!wsConnected && (
            <Alert severity="warning" sx={{ mb: 1 }}>
              WebSocket连接已断开，正在尝试重连...
            </Alert>
          )}

          {/* 群聊界面 */}
          <GroupChatInterface
            messages={messages}
            selectedModels={selectedModels}
            contextSize={contextSize}
            modelStatuses={modelStatuses}
            onSendMessage={handleSendMessage}
            onReopenSelection={() => setIsSelectionOpen(true)}
          />
        </Paper>
      )}
    </Container>
  );
};

export default GroupChatPage;
```

#### 1.2 大模型选择对话框 (ModelSelectionDialog.jsx)

```jsx
import React, { useState, useEffect } from 'react';
import {
  Dialog,
  DialogTitle,
  DialogContent,
  DialogActions,
  Button,
  Grid,
  FormControl,
  FormLabel,
  RadioGroup,
  FormControlLabel,
  Radio,
  TextField,
  Divider,
  Typography,
  Box
} from '@mui/material';
import InfoCard from './InfoCard';
import { getAvailableModels } from '../services/api';

const ModelSelectionDialog = ({ open, onClose, onConfirm }) => {
  const [availableModels, setAvailableModels] = useState([]);
  const [selectedModels, setSelectedModels] = useState([]);
  const [promptMode, setPromptMode] = useState('unified'); // 'unified' | 'individual'
  const [unifiedPrompt, setUnifiedPrompt] = useState('你们可以自己给自己一个角色，性格随机自选，参与这场群聊讨论');
  const [individualPrompts, setIndividualPrompts] = useState({});

  // 加载可用模型
  useEffect(() => {
    if (open) {
      loadAvailableModels();
    }
  }, [open]);

  const loadAvailableModels = async () => {
    try {
      const models = await getAvailableModels();
      setAvailableModels(models);
    } catch (error) {
      console.error('Failed to load available models:', error);
    }
  };

  // 处理模型选择
  const handleModelToggle = (model) => {
    setSelectedModels(prev => {
      const isSelected = prev.some(m => m.id === model.id);
      if (isSelected) {
        // 移除模型
        const newSelected = prev.filter(m => m.id !== model.id);
        // 同时移除个性化提示词
        const newPrompts = { ...individualPrompts };
        delete newPrompts[model.id];
        setIndividualPrompts(newPrompts);
        return newSelected;
      } else {
        // 添加模型
        return [...prev, model];
      }
    });
  };

  // 处理个性化提示词变更
  const handleIndividualPromptChange = (modelId, prompt) => {
    setIndividualPrompts(prev => ({
      ...prev,
      [modelId]: prompt
    }));
  };

  // 确认选择
  const handleConfirm = () => {
    if (selectedModels.length === 0) {
      alert('请至少选择一个大模型');
      return;
    }

    const systemPrompts = promptMode === 'unified' 
      ? { mode: 'unified', prompt: unifiedPrompt }
      : { mode: 'individual', prompts: individualPrompts };

    onConfirm(selectedModels, systemPrompts);
  };

  return (
    <Dialog 
      open={open} 
      onClose={onClose}
      maxWidth="md"
      fullWidth
      PaperProps={{
        sx: { minHeight: '70vh' }
      }}
    >
      <DialogTitle>
        <Typography variant="h5" component="div">
          选择群聊参与的大模型
        </Typography>
        <Typography variant="body2" color="text.secondary" sx={{ mt: 1 }}>
          选择您希望参与群聊的AI大模型，并配置它们的系统提示词
        </Typography>
      </DialogTitle>

      <DialogContent dividers>
        {/* 模型选择区域 */}
        <Box sx={{ mb: 3 }}>
          <Typography variant="h6" gutterBottom>
            可用模型 ({selectedModels.length} 已选择)
          </Typography>
          <Grid container spacing={2}>
            {availableModels.map((model) => (
              <Grid item xs={12} sm={6} md={4} key={model.id}>
                <InfoCard
                  model={model}
                  selected={selectedModels.some(m => m.id === model.id)}
                  onToggle={() => handleModelToggle(model)}
                  variant="selection"
                />
              </Grid>
            ))}
          </Grid>
        </Box>

        <Divider sx={{ my: 3 }} />

        {/* 系统提示词配置 */}
        <Box>
          <Typography variant="h6" gutterBottom>
            系统提示词配置
          </Typography>
          
          <FormControl component="fieldset" sx={{ mb: 2 }}>
            <FormLabel component="legend">配置模式</FormLabel>
            <RadioGroup
              value={promptMode}
              onChange={(e) => setPromptMode(e.target.value)}
              row
            >
              <FormControlLabel 
                value="unified" 
                control={<Radio />} 
                label="统一配置" 
              />
              <FormControlLabel 
                value="individual" 
                control={<Radio />} 
                label="个性化配置" 
              />
            </RadioGroup>
          </FormControl>

          {promptMode === 'unified' ? (
            <TextField
              fullWidth
              multiline
              rows={3}
              label="统一系统提示词"
              value={unifiedPrompt}
              onChange={(e) => setUnifiedPrompt(e.target.value)}
              placeholder="为所有选中的大模型设置统一的系统提示词..."
              helperText="此提示词将应用于所有选中的大模型"
            />
          ) : (
            <Box>
              <Typography variant="body2" color="text.secondary" sx={{ mb: 2 }}>
                为每个选中的大模型单独配置系统提示词：
              </Typography>
              {selectedModels.map((model) => (
                <TextField
                  key={model.id}
                  fullWidth
                  multiline
                  rows={2}
                  label={`${model.name} 的系统提示词`}
                  value={individualPrompts[model.id] || ''}
                  onChange={(e) => handleIndividualPromptChange(model.id, e.target.value)}
                  placeholder={`为 ${model.name} 设置个性化提示词，例如："你是一个医生，性格温和，参与这场群聊讨论"`}
                  sx={{ mb: 2 }}
                />
              ))}
            </Box>
          )}
        </Box>
      </DialogContent>

      <DialogActions sx={{ p: 2 }}>
        <Button onClick={onClose} color="inherit">
          取消
        </Button>
        <Button 
          onClick={handleConfirm} 
          variant="contained"
          disabled={selectedModels.length === 0}
        >
          开始群聊 ({selectedModels.length} 个模型)
        </Button>
      </DialogActions>
    </Dialog>
  );
};

export default ModelSelectionDialog;
```

#### 1.3 群聊界面组件 (GroupChatInterface.jsx)

```jsx
import React, { useState, useRef, useEffect } from 'react';
import {
  Box,
  Paper,
  Typography,
  TextField,
  IconButton,
  Chip,
  LinearProgress,
  Divider,
  Avatar,
  List,
  ListItem,
  ListItemAvatar,
  ListItemText,
  Button,
  Tooltip
} from '@mui/material';
import {
  Send as SendIcon,
  Settings as SettingsIcon,
  Person as PersonIcon,
  SmartToy as BotIcon
} from '@mui/icons-material';

const GroupChatInterface = ({
  messages,
  selectedModels,
  contextSize,
  modelStatuses,
  onSendMessage,
  onReopenSelection
}) => {
  const [inputValue, setInputValue] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const messagesEndRef = useRef(null);

  // 自动滚动到底部
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  // 发送消息
  const handleSend = async () => {
    if (!inputValue.trim() || isLoading) return;

    const message = inputValue.trim();
    setInputValue('');
    setIsLoading(true);

    try {
      await onSendMessage(message);
    } catch (error) {
      console.error('Failed to send message:', error);
    } finally {
      setIsLoading(false);
    }
  };

  // 处理回车发送
  const handleKeyPress = (e) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  };

  // 获取模型信息
  const getModelInfo = (modelId) => {
    return selectedModels.find(m => m.id === modelId) || { name: 'Unknown', color: '#666' };
  };

  // 计算上下文使用率
  const getContextUsagePercent = (modelId) => {
    const status = modelStatuses[modelId];
    if (!status || !status.maxContext) return 0;
    return (status.usedContext / status.maxContext) * 100;
  };

  return (
    <Box sx={{ height: '100%', display: 'flex', flexDirection: 'column' }}>
      {/* 头部信息栏 */}
      <Box sx={{ p: 2, borderBottom: 1, borderColor: 'divider' }}>
        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 1 }}>
          <Typography variant="h6">
            群聊模式 ({selectedModels.length} 个模型)
          </Typography>
          <Button
            startIcon={<SettingsIcon />}
            onClick={onReopenSelection}
            size="small"
          >
            重新配置
          </Button>
        </Box>

        {/* 参与模型列表 */}
        <Box sx={{ display: 'flex', flexWrap: 'wrap', gap: 1, mb: 2 }}>
          {selectedModels.map((model) => (
            <Tooltip 
              key={model.id}
              title={`${model.name} - ${modelStatuses[model.id]?.status || '准备中'}`}
            >
              <Chip
                avatar={<Avatar sx={{ bgcolor: model.color }}>{model.name[0]}</Avatar>}
                label={model.name}
                variant="outlined"
                size="small"
                color={modelStatuses[model.id]?.status === 'active' ? 'primary' : 'default'}
              />
            </Tooltip>
          ))}
        </Box>

        {/* 上下文信息 */}
        <Box>
          <Typography variant="body2" color="text.secondary" gutterBottom>
            共享上下文大小: {contextSize} tokens
          </Typography>
          
          {/* 各模型上下文状态 */}
          <Box sx={{ display: 'flex', flexWrap: 'wrap', gap: 2 }}>
            {selectedModels.map((model) => {
              const status = modelStatuses[model.id];
              const usagePercent = getContextUsagePercent(model.id);
              
              return (
                <Box key={model.id} sx={{ minWidth: 120 }}>
                  <Typography variant="caption" display="block">
                    {model.name}
                  </Typography>
                  <LinearProgress
                    variant="determinate"
                    value={usagePercent}
                    sx={{ height: 4, borderRadius: 2 }}
                    color={usagePercent > 80 ? 'error' : usagePercent > 60 ? 'warning' : 'primary'}
                  />
                  <Typography variant="caption" color="text.secondary">
                    {status?.usedContext || 0}/{status?.maxContext || 0}
                  </Typography>
                </Box>
              );
            })}
          </Box>
        </Box>
      </Box>

      {/* 消息列表 */}
      <Box sx={{ flex: 1, overflow: 'auto', p: 1 }}>
        <List>
          {messages.map((message) => {
            const isUser = message.type === 'user';
            const modelInfo = isUser ? null : getModelInfo(message.modelId);

            return (
              <ListItem
                key={message.id}
                sx={{
                  flexDirection: isUser ? 'row-reverse' : 'row',
                  alignItems: 'flex-start',
                  mb: 1
                }}
              >
                <ListItemAvatar sx={{ minWidth: 40 }}>
                  <Avatar sx={{ 
                    bgcolor: isUser ? 'primary.main' : (modelInfo?.color || 'secondary.main'),
                    width: 32,
                    height: 32
                  }}>
                    {isUser ? <PersonIcon /> : <BotIcon />}
                  </Avatar>
                </ListItemAvatar>

                <ListItemText
                  primary={
                    <Box sx={{ 
                      display: 'flex', 
                      alignItems: 'center', 
                      gap: 1,
                      flexDirection: isUser ? 'row-reverse' : 'row'
                    }}>
                      <Typography variant="subtitle2">
                        {isUser ? '我' : modelInfo?.name || 'AI'}
                      </Typography>
                      <Typography variant="caption" color="text.secondary">
                        {message.timestamp.toLocaleTimeString()}
                      </Typography>
                    </Box>
                  }
                  secondary={
                    <Paper
                      elevation={1}
                      sx={{
                        p: 1.5,
                        mt: 0.5,
                        bgcolor: isUser ? 'primary.light' : 'background.paper',
                        color: isUser ? 'primary.contrastText' : 'text.primary',
                        borderRadius: 2,
                        maxWidth: '80%',
                        ml: isUser ? 'auto' : 0,
                        mr: isUser ? 0 : 'auto'
                      }}
                    >
                      <Typography variant="body2" sx={{ whiteSpace: 'pre-wrap' }}>
                        {message.content}
                      </Typography>
                    </Paper>
                  }
                  sx={{ 
                    textAlign: isUser ? 'right' : 'left',
                    ml: isUser ? 2 : 0,
                    mr: isUser ? 0 : 2
                  }}
                />
              </ListItem>
            );
          })}
        </List>
        <div ref={messagesEndRef} />
      </Box>

      {/* 输入区域 */}
      <Box sx={{ p: 2, borderTop: 1, borderColor: 'divider' }}>
        {isLoading && (
          <LinearProgress sx={{ mb: 1 }} />
        )}
        
        <Box sx={{ display: 'flex', gap: 1 }}>
          <TextField
            fullWidth
            multiline
            maxRows={4}
            value={inputValue}
            onChange={(e) => setInputValue(e.target.value)}
            onKeyPress={handleKeyPress}
            placeholder="输入消息参与群聊讨论..."
            disabled={isLoading}
            variant="outlined"
            size="small"
          />
          <IconButton
            onClick={handleSend}
            disabled={!inputValue.trim() || isLoading}
            color="primary"
            sx={{ alignSelf: 'flex-end' }}
          >
            <SendIcon />
          </IconButton>
        </Box>
      </Box>
    </Box>
  );
};

export default GroupChatInterface;
```

#### 1.4 WebSocket管理器 (WebSocketManager.js)

```javascript
class WebSocketManager {
  constructor(options = {}) {
    this.url = options.url || `ws://localhost:8000/ws/group-chat`;
    this.onConnect = options.onConnect || (() => {});
    this.onDisconnect = options.onDisconnect || (() => {});
    this.onMessage = options.onMessage || (() => {});
    this.onError = options.onError || (() => {});
    
    this.ws = null;
    this.reconnectAttempts = 0;
    this.maxReconnectAttempts = 5;
    this.reconnectDelay = 1000;
    this.isConnecting = false;
  }

  async connect() {
    if (this.isConnecting || (this.ws && this.ws.readyState === WebSocket.OPEN)) {
      return;
    }

    this.isConnecting = true;

    try {
      this.ws = new WebSocket(this.url);
      
      this.ws.onopen = () => {
        console.log('WebSocket connected');
        this.isConnecting = false;
        this.reconnectAttempts = 0;
        this.onConnect();
      };

      this.ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          this.onMessage(message);
        } catch (error) {
          console.error('Failed to parse WebSocket message:', error);
        }
      };

      this.ws.onclose = (event) => {
        console.log('WebSocket disconnected:', event.code, event.reason);
        this.isConnecting = false;
        this.onDisconnect();
        
        // 自动重连
        if (this.reconnectAttempts < this.maxReconnectAttempts) {
          setTimeout(() => {
            this.reconnectAttempts++;
            this.connect();
          }, this.reconnectDelay * Math.pow(2, this.reconnectAttempts));
        }
      };

      this.ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        this.isConnecting = false;
        this.onError(error);
      };

    } catch (error) {
      this.isConnecting = false;
      this.onError(error);
    }
  }

  disconnect() {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }

  async initializeGroupChat(models, systemPrompts) {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new Error('WebSocket not connected');
    }

    const message = {
      type: 'initialize_group_chat',
      data: {
        models: models.map(m => ({
          id: m.id,
          name: m.name,
          provider: m.provider
        })),
        systemPrompts
      }
    };

    this.ws.send(JSON.stringify(message));
  }

  async sendMessage(content) {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new Error('WebSocket not connected');
    }

    const message = {
      type: 'user_message',
      data: {
        content,
        timestamp: new Date().toISOString()
      }
    };

    this.ws.send(JSON.stringify(message));
  }

  isConnected() {
    return this.ws && this.ws.readyState === WebSocket.OPEN;
  }
}

export default WebSocketManager;
```

### 2. 后端实现

#### 2.1 WebSocket处理器 (websocket_handler.py)

```python
import asyncio
import json
import logging
from typing import Dict, List, Set
from fastapi import WebSocket, WebSocketDisconnect
from datetime import datetime

from .models.chat_models import ChatMessage, GroupChatSession
from .services.model_service import ModelService
from .services.context_service import ContextService

logger = logging.getLogger(__name__)

class ConnectionManager:
    """WebSocket连接管理器"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.group_sessions: Dict[str, GroupChatSession] = {}
    
    async def connect(self, websocket: WebSocket, session_id: str):
        """建立WebSocket连接"""
        await websocket.accept()
        self.active_connections[session_id] = websocket
        logger.info(f"WebSocket connected: {session_id}")
    
    def disconnect(self, session_id: str):
        """断开WebSocket连接"""
        if session_id in self.active_connections:
            del self.active_connections[session_id]
        if session_id in self.group_sessions:
            del self.group_sessions[session_id]
        logger.info(f"WebSocket disconnected: {session_id}")
    
    async def send_message(self, session_id: str, message: dict):
        """发送消息到指定会话"""
        if session_id in self.active_connections:
            websocket = self.active_connections[session_id]
            try:
                await websocket.send_text(json.dumps(message))
            except Exception as e:
                logger.error(f"Failed to send message to {session_id}: {e}")
                self.disconnect(session_id)

class GroupChatHandler:
    """群聊处理器"""
    
    def __init__(self):
        self.connection_manager = ConnectionManager()
        self.model_service = ModelService()
        self.context_service = ContextService()
    
    async def handle_websocket(self, websocket: WebSocket, session_id: str):
        """处理WebSocket连接"""
        await self.connection_manager.connect(websocket, session_id)
        
        try:
            while True:
                # 接收消息
                data = await websocket.receive_text()
                message = json.loads(data)
                
                # 处理不同类型的消息
                await self.handle_message(session_id, message)
                
        except WebSocketDisconnect:
            self.connection_manager.disconnect(session_id)
        except Exception as e:
            logger.error(f"WebSocket error for {session_id}: {e}")
            self.connection_manager.disconnect(session_id)
    
    async def handle_message(self, session_id: str, message: dict):
        """处理接收到的消息"""
        message_type = message.get('type')
        data = message.get('data', {})
        
        if message_type == 'initialize_group_chat':
            await self.initialize_group_chat(session_id, data)
        elif message_type == 'user_message':
            await self.handle_user_message(session_id, data)
        else:
            logger.warning(f"Unknown message type: {message_type}")
    
    async def initialize_group_chat(self, session_id: str, data: dict):
        """初始化群聊会话"""
        try:
            models = data.get('models', [])
            system_prompts = data.get('systemPrompts', {})
            
            # 创建群聊会话
            session = GroupChatSession(
                session_id=session_id,
                models=models,
                system_prompts=system_prompts,
                created_at=datetime.now()
            )
            
            self.connection_manager.group_sessions[session_id] = session
            
            # 初始化各个模型的上下文
            for model in models:
                await self.context_service.initialize_model_context(
                    session_id, 
                    model['id'], 
                    system_prompts
                )
            
            # 发送初始化成功消息
            await self.connection_manager.send_message(session_id, {
                'type': 'initialization_complete',
                'data': {
                    'session_id': session_id,
                    'models': models
                }
            })
            
            # 发送初始上下文状态
            await self.send_context_update(session_id)
            
        except Exception as e:
            logger.error(f"Failed to initialize group chat: {e}")
            await self.connection_manager.send_message(session_id, {
                'type': 'error',
                'data': {'message': f'初始化群聊失败: {str(e)}'}
            })
    
    async def handle_user_message(self, session_id: str, data: dict):
        """处理用户消息"""
        try:
            content = data.get('content', '').strip()
            if not content:
                return
            
            session = self.connection_manager.group_sessions.get(session_id)
            if not session:
                await self.connection_manager.send_message(session_id, {
                    'type': 'error',
                    'data': {'message': '群聊会话未初始化'}
                })
                return
            
            # 创建用户消息
            user_message = ChatMessage(
                role='user',
                content=content,
                timestamp=datetime.now()
            )
            
            # 添加到共享上下文
            await self.context_service.add_message(session_id, user_message)
            
            # 发送上下文更新
            await self.send_context_update(session_id)
            
            # 并发处理所有模型的响应
            tasks = []
            for model in session.models:
                task = asyncio.create_task(
                    self.process_model_response(session_id, model, user_message)
                )
                tasks.append(task)
            
            # 等待所有模型响应完成
            await asyncio.gather(*tasks, return_exceptions=True)
            
        except Exception as e:
            logger.error(f"Failed to handle user message: {e}")
            await self.connection_manager.send_message(session_id, {
                'type': 'error',
                'data': {'message': f'处理消息失败: {str(e)}'}
            })
    
    async def process_model_response(self, session_id: str, model: dict, user_message: ChatMessage):
        """处理单个模型的响应"""
        try:
            model_id = model['id']
            model_name = model['name']
            
            # 更新模型状态为处理中
            await self.send_model_status_update(session_id, model_id, 'processing')
            
            # 获取模型的上下文
            context = await self.context_service.get_model_context(session_id, model_id)
            
            # 调用模型生成响应
            response = await self.model_service.generate_response(
                model_id=model_id,
                messages=context,
                stream=False
            )
            
            if response and response.content:
                # 创建AI响应消息
                ai_message = ChatMessage(
                    role='assistant',
                    content=response.content,
                    timestamp=datetime.now(),
                    model_id=model_id,
                    model_name=model_name
                )
                
                # 添加到共享上下文
                await self.context_service.add_message(session_id, ai_message)
                
                # 发送响应到前端
                await self.connection_manager.send_message(session_id, {
                    'type': 'model_response',
                    'data': {
                        'content': response.content,
                        'modelId': model_id,
                        'modelName': model_name,
                        'timestamp': ai_message.timestamp.isoformat()
                    }
                })
                
                # 更新上下文状态
                await self.send_context_update(session_id)
            
            # 更新模型状态为活跃
            await self.send_model_status_update(session_id, model_id, 'active')
            
        except Exception as e:
            logger.error(f"Model {model['id']} response failed: {e}")
            await self.send_model_status_update(session_id, model['id'], 'error')
            await self.connection_manager.send_message(session_id, {
                'type': 'model_error',
                'data': {
                    'modelId': model['id'],
                    'modelName': model['name'],
                    'error': str(e)
                }
            })
    
    async def send_context_update(self, session_id: str):
        """发送上下文更新"""
        try:
            context_info = await self.context_service.get_context_info(session_id)
            
            await self.connection_manager.send_message(session_id, {
                'type': 'context_update',
                'data': {
                    'contextSize': context_info['total_tokens'],
                    'messageCount': context_info['message_count']
                }
            })
            
        except Exception as e:
            logger.error(f"Failed to send context update: {e}")
    
    async def send_model_status_update(self, session_id: str, model_id: str, status: str):
        """发送模型状态更新"""
        try:
            # 获取模型的上下文使用情况
            model_context = await self.context_service.get_model_context_usage(session_id, model_id)
            
            await self.connection_manager.send_message(session_id, {
                'type': 'model_status',
                'data': {
                    'modelId': model_id,
                    'status': status,
                    'usedContext': model_context.get('used_tokens', 0),
                    'maxContext': model_context.get('max_tokens', 0)
                }
            })
            
        except Exception as e:
            logger.error(f"Failed to send model status update: {e}")

# 全局处理器实例
group_chat_handler = GroupChatHandler()
```

#### 2.2 上下文服务 (context_service.py)

```python
import asyncio
from typing import Dict, List, Optional
from datetime import datetime

from .models.chat_models import ChatMessage
from .utils.token_counter import count_tokens

class ContextService:
    """上下文管理服务"""
    
    def __init__(self):
        # 存储会话的共享上下文
        self.shared_contexts: Dict[str, List[ChatMessage]] = {}
        # 存储各模型的系统提示词
        self.model_system_prompts: Dict[str, Dict[str, str]] = {}
        # 存储模型的上下文限制
        self.model_limits: Dict[str, int] = {
            'gpt-4': 8192,
            'gpt-3.5-turbo': 4096,
            'claude-3': 100000,
            'gemini-pro': 32768,
            # 添加更多模型的限制
        }
    
    async def initialize_model_context(self, session_id: str, model_id: str, system_prompts: dict):
        """初始化模型上下文"""
        if session_id not in self.shared_contexts:
            self.shared_contexts[session_id] = []
        
        if session_id not in self.model_system_prompts:
            self.model_system_prompts[session_id] = {}
        
        # 设置系统提示词
        if system_prompts.get('mode') == 'unified':
            prompt = system_prompts.get('prompt', '')
        else:
            prompt = system_prompts.get('prompts', {}).get(model_id, '')
        
        self.model_system_prompts[session_id][model_id] = prompt
    
    async def add_message(self, session_id: str, message: ChatMessage):
        """添加消息到共享上下文"""
        if session_id not in self.shared_contexts:
            self.shared_contexts[session_id] = []
        
        self.shared_contexts[session_id].append(message)
        
        # 清理过长的上下文（保留最近的消息）
        await self.cleanup_context(session_id)
    
    async def get_model_context(self, session_id: str, model_id: str) -> List[dict]:
        """获取特定模型的上下文"""
        if session_id not in self.shared_contexts:
            return []
        
        messages = []
        
        # 添加系统提示词
        system_prompt = self.model_system_prompts.get(session_id, {}).get(model_id, '')
        if system_prompt:
            messages.append({
                'role': 'system',
                'content': system_prompt
            })
        
        # 添加共享上下文消息
        shared_messages = self.shared_contexts[session_id]
        for msg in shared_messages:
            messages.append({
                'role': msg.role,
                'content': msg.content
            })
        
        # 根据模型限制裁剪上下文
        return await self.trim_context_for_model(model_id, messages)
    
    async def trim_context_for_model(self, model_id: str, messages: List[dict]) -> List[dict]:
        """根据模型限制裁剪上下文"""
        max_tokens = self.model_limits.get(model_id, 4096)
        
        # 保留系统消息
        system_messages = [msg for msg in messages if msg['role'] == 'system']
        other_messages = [msg for msg in messages if msg['role'] != 'system']
        
        # 计算系统消息的token数
        system_tokens = sum(count_tokens(msg['content']) for msg in system_messages)
        available_tokens = max_tokens - system_tokens - 500  # 预留500tokens给响应
        
        if available_tokens <= 0:
            return system_messages
        
        # 从最新消息开始，逐步添加到上下文中
        selected_messages = []
        current_tokens = 0
        
        for msg in reversed(other_messages):
            msg_tokens = count_tokens(msg['content'])
            if current_tokens + msg_tokens <= available_tokens:
                selected_messages.insert(0, msg)
                current_tokens += msg_tokens
            else:
                break
        
        return system_messages + selected_messages
    
    async def get_context_info(self, session_id: str) -> dict:
        """获取上下文信息"""
        if session_id not in self.shared_contexts:
            return {'total_tokens': 0, 'message_count': 0}
        
        messages = self.shared_contexts[session_id]
        total_tokens = sum(count_tokens(msg.content) for msg in messages)
        
        return {
            'total_tokens': total_tokens,
            'message_count': len(messages)
        }
    
    async def get_model_context_usage(self, session_id: str, model_id: str) -> dict:
        """获取模型的上下文使用情况"""
        context = await self.get_model_context(session_id, model_id)
        used_tokens = sum(count_tokens(msg['content']) for msg in context)
        max_tokens = self.model_limits.get(model_id, 4096)
        
        return {
            'used_tokens': used_tokens,
            'max_tokens': max_tokens,
            'usage_percent': (used_tokens / max_tokens) * 100 if max_tokens > 0 else 0
        }
    
    async def cleanup_context(self, session_id: str, max_messages: int = 100):
        """清理过长的上下文"""
        if session_id not in self.shared_contexts:
            return
        
        messages = self.shared_contexts[session_id]
        if len(messages) > max_messages:
            # 保留最近的消息
            self.shared_contexts[session_id] = messages[-max_messages:]
    
    def clear_session_context(self, session_id: str):
        """清除会话上下文"""
        if session_id in self.shared_contexts:
            del self.shared_contexts[session_id]
        if session_id in self.model_system_prompts:
            del self.model_system_prompts[session_id]
```

#### 2.3 路由配置 (main.py 添加)

```python
from fastapi import FastAPI, WebSocket
from .websocket_handler import group_chat_handler

app = FastAPI()

@app.websocket("/ws/group-chat")
async def websocket_endpoint(websocket: WebSocket):
    """群聊WebSocket端点"""
    import uuid
    session_id = str(uuid.uuid4())
    await group_chat_handler.handle_websocket(websocket, session_id)

@app.websocket("/ws/group-chat/{session_id}")
async def websocket_endpoint_with_id(websocket: WebSocket, session_id: str):
    """带会话ID的群聊WebSocket端点"""
    await group_chat_handler.handle_websocket(websocket, session_id)
```

## 关键技术要点

### 1. WebSocket通信机制
- **连接管理**：维护活跃连接和会话状态
- **消息路由**：根据消息类型分发到相应处理器
- **错误处理**：自动重连和错误恢复机制
- **并发处理**：支持多个模型同时处理消息

### 2. 上下文共享策略
- **统一存储**：所有模型共享同一个消息历史
- **智能裁剪**：根据各模型的上下文限制动态裁剪
- **实时更新**：消息添加后立即更新所有相关状态
- **内存管理**：定期清理过长的上下文历史

### 3. 系统提示词配置
- **灵活模式**：支持统一和个性化两种配置方式
- **动态应用**：在运行时应用到各个模型的上下文
- **持久化**：会话期间保持提示词配置

### 4. 性能优化
- **异步处理**：所有IO操作使用异步模式
- **并发响应**：多个模型并行生成响应
- **智能缓存**：缓存频繁访问的上下文数据
- **资源管理**：及时清理无用的连接和会话

## 部署和测试

### 1. 开发环境启动

```bash
# 启动后端服务
cd api-server
.\.venv\Scripts\activate
python .\start_server.py

# 启动前端服务
cd avatar-react
npm run dev
```

### 2. 功能测试清单

#### 基础功能测试
- [ ] 群聊页面正常加载
- [ ] 模型选择对话框正常显示
- [ ] 可以选择多个大模型
- [ ] 系统提示词配置功能正常
- [ ] WebSocket连接建立成功

#### 核心功能测试
- [ ] 用户消息发送成功
- [ ] 多个模型能够接收并响应消息
- [ ] 消息显示顺序正确（最先回复的最先显示）
- [ ] 上下文大小实时更新
- [ ] 各模型上下文使用率正确显示

#### 高级功能测试
- [ ] 统一系统提示词模式工作正常
- [ ] 个性化系统提示词模式工作正常
- [ ] 模型状态实时更新
- [ ] WebSocket断线重连功能
- [ ] 长对话的上下文管理

#### 边界情况测试
- [ ] 网络断开时的处理
- [ ] 模型响应失败时的处理
- [ ] 上下文超出限制时的裁剪
- [ ] 大量并发消息的处理
- [ ] 内存使用的控制

### 3. 性能基准

- **响应时间**：用户消息到第一个AI回复 < 3秒
- **并发支持**：单个会话支持最多10个模型同时参与
- **上下文管理**：支持最多1000条消息的历史记录
- **内存使用**：单个会话内存占用 < 100MB

## 注意事项和最佳实践

### 1. 代码质量保证
- **不影响现有功能**：所有新增代码都应该是独立的模块
- **向后兼容**：确保单聊功能不受影响
- **错误处理**：完善的异常捕获和用户友好的错误提示
- **代码复用**：尽可能复用现有的组件和服务

### 2. 用户体验优化
- **加载状态**：清晰的加载指示器和状态反馈
- **响应式设计**：适配不同屏幕尺寸
- **键盘快捷键**：支持Enter发送消息
- **消息格式**：支持多行文本和特殊字符

### 3. 安全考虑
- **输入验证**：对用户输入进行适当的验证和清理
- **连接安全**：WebSocket连接的身份验证
- **资源限制**：防止恶意用户消耗过多资源
- **数据隐私**：不在日志中记录敏感的对话内容

### 4. 扩展性设计
- **模块化架构**：便于添加新的功能和模型
- **配置化**：关键参数通过配置文件管理
- **插件机制**：支持第三方模型的接入
- **监控和日志**：完善的运行状态监控

## 总结

群聊功能是一个复杂的实时通信系统，涉及前后端的深度协作。通过WebSocket实现实时通信，通过共享上下文机制实现多模型协作，通过灵活的配置系统实现个性化体验。

该设计方案充分考虑了性能、可扩展性和用户体验，同时保持了与现有系统的兼容性。在实施过程中，建议采用渐进式开发方式，先实现核心功能，再逐步完善高级特性。

通过这个详细的文档，开发团队可以按照既定的架构和规范，高效地实现群聊功能，为用户提供优质的多模型对话体验。